from django.shortcuts import render
from django.http import JsonResponse, HttpResponse
from django.views.decorators.csrf import csrf_exempt
import json
import os
from .scraper import AdvancedVapeScraper
import threading
import pandas as pd
from uuid import uuid4

def index(request):
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ"""
    return render(request, 'crawler/index.html')

@csrf_exempt
def start_scraping(request):
    """Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ÛŒÙ† Ø³Ø§ÛŒØª"""
    if request.method == 'POST':
        try:
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø§Ø² Ø¯Ø±Ø®ÙˆØ§Ø³Øª
            data = json.loads(request.body)
            sites = data.get('sites', [])
            
            if not sites:
                return JsonResponse({'success': False, 'error': 'Ø³Ø§ÛŒØªÛŒ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡'})
            
            print(f"ğŸ”§ Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ Ø¨Ø±Ø§ÛŒ {len(sites)} Ø³Ø§ÛŒØª: {sites}")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø± Ø¬Ø¯ÛŒØ¯
            scraper = AdvancedVapeScraper()
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            def run_scraping():
                try:
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ú†Ù†Ø¯Ø³Ø§ÛŒØªÛŒ
                    result = scraper.scrape_multiple_sites(sites)
                    scraper.close()
                    print(f"âœ… Ù†ØªÛŒØ¬Ù‡ Ø§Ø³Ú©Ø±Ù¾: {result}")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾: {e}")
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± thread Ø¬Ø¯ÛŒØ¯
            thread = threading.Thread(target=run_scraping)
            thread.daemon = True
            thread.start()
            
            # ÙÙˆØ±Ø§Ù‹ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡
            return JsonResponse({
                'success': True, 
                'message': f'Ø§Ø³Ú©Ø±Ù¾ Ø¨Ø±Ø§ÛŒ {len(sites)} Ø³Ø§ÛŒØª Ø´Ø±ÙˆØ¹ Ø´Ø¯',
                'job_id': scraper.job_id,
                'sites_count': len(sites)
            })
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾: {e}")
            return JsonResponse({'success': False, 'error': str(e)})
    
    return JsonResponse({'success': False, 'error': 'Ù…ØªØ¯ ØºÛŒØ±Ù…Ø¬Ø§Ø²'})

@csrf_exempt
def start_scraping_all(request):
    """Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… 7 Ø³Ø§ÛŒØª Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø±"""
    if request.method == 'POST':
        try:
            print("ğŸ”§ Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… 7 Ø³Ø§ÛŒØª")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø± Ø¬Ø¯ÛŒØ¯
            scraper = AdvancedVapeScraper()
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            def run_scraping():
                try:
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ ØªÙ…Ø§Ù… Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§
                    result = scraper.scrape_all_sites()
                    scraper.close()
                    print(f"âœ… Ù†ØªÛŒØ¬Ù‡ Ø§Ø³Ú©Ø±Ù¾: {result}")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾: {e}")
            
            # Ø§Ø¬Ø±Ø§ Ø¯Ø± thread Ø¬Ø¯ÛŒØ¯
            thread = threading.Thread(target=run_scraping)
            thread.daemon = True
            thread.start()
            
            # ÙÙˆØ±Ø§Ù‹ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡
            return JsonResponse({
                'success': True, 
                'message': 'Ø§Ø³Ú©Ø±Ù¾ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ 7 Ø³Ø§ÛŒØª Ø´Ø±ÙˆØ¹ Ø´Ø¯',
                'job_id': scraper.job_id,
                'sites_count': 7
            })
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾: {e}")
            return JsonResponse({'success': False, 'error': str(e)})
    
    return JsonResponse({'success': False, 'error': 'Ù…ØªØ¯ ØºÛŒØ±Ù…Ø¬Ø§Ø²'})

def get_progress(request):
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒØ´Ø±ÙØª"""
    try:
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª
        if not os.path.exists('tmp_jobs'):
            return JsonResponse({
                'status': 'Ø¢Ù…Ø§Ø¯Ù‡',
                'page': 0,
                'total_pages': 0,
                'products_count': 0,
                'total_products': 0,
                'current_site': ''
            })
            
        status_files = [f for f in os.listdir('tmp_jobs') if f.endswith('_status.json')]
        if status_files:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„
            latest_file = max(status_files, key=lambda f: os.path.getctime(os.path.join('tmp_jobs', f)))
            with open(f'tmp_jobs/{latest_file}', 'r', encoding='utf-8') as f:
                status_data = json.load(f)
                return JsonResponse(status_data)
        
        return JsonResponse({
            'status': 'Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ...',
            'page': 0,
            'total_pages': 0,
            'products_count': 0,
            'total_products': 0,
            'current_site': ''
        })
        
    except Exception as e:
        return JsonResponse({
            'status': f'Ø®Ø·Ø§: {str(e)}',
            'products_count': 0,
            'total_products': 0,
            'current_site': ''
        })

def preview_products(request, job_id):
    """Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    try:
        json_file = f'tmp_jobs/{job_id}.json'
        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            products = data.get('products', [])[:20]  # ÙÙ‚Ø· 20 Ù…Ø­ØµÙˆÙ„ Ø§ÙˆÙ„
            
            # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØª
            products_by_site = {}
            for product in products:
                site = product.get('site', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                if site not in products_by_site:
                    products_by_site[site] = []
                products_by_site[site].append(product)
            
            return render(request, 'crawler/preview.html', {
                'products': products,
                'products_by_site': products_by_site,
                'job_id': job_id,
                'total_products': len(data.get('products', [])),
                'sites_count': len(products_by_site)
            })
        else:
            return render(request, 'crawler/preview.html', {
                'error': 'Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯',
                'products': [],
                'products_by_site': {},
                'total_products': 0,
                'sites_count': 0
            })
    except Exception as e:
        return render(request, 'crawler/preview.html', {
            'error': str(e),
            'products': [],
            'products_by_site': {},
            'total_products': 0,
            'sites_count': 0
        })

def download_excel(request, job_id):
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„"""
    try:
        excel_file = f'tmp_jobs/{job_id}.xlsx'
        if os.path.exists(excel_file):
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
            with open(excel_file, 'rb') as f:
                response = HttpResponse(
                    f.read(),
                    content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
                response['Content-Disposition'] = f'attachment; filename="products_{job_id}.xlsx"'
                return response
        else:
            return JsonResponse({'success': False, 'error': 'ÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

def test_view(request):
    """ØµÙØ­Ù‡ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆØ±"""
    return JsonResponse({
        'status': 'OK', 
        'message': 'Ø³Ø±ÙˆØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯',
        'endpoints': {
            'home': '/',
            'test': '/test/',
            'start_scraping': '/start-scraping/',
            'start_scraping_all': '/start-scraping-all/',
            'progress': '/progress/',
            'preview': '/preview/<job_id>/',
            'download': '/download/<job_id>/',
            'job_status': '/job-status/<job_id>/',
            'list_jobs': '/list-jobs/'
        },
        'supported_sites': [
            'Vape60shop22.com',
            'Tajvape12.com', 
            'Vapoursdaily14.com',
            'Digizima19.com',
            'Smokcenter16.com',
            'Digighelioon.com',
            'Dokhanmarket3.com'
        ]
    })

def get_job_status(request, job_id):
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª ÛŒÚ© Job Ø®Ø§Øµ"""
    try:
        status_file = f'tmp_jobs/{job_id}_status.json'
        if os.path.exists(status_file):
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
                return JsonResponse(status_data)
        else:
            return JsonResponse({
                'success': False,
                'error': 'Job ÛŒØ§ÙØª Ù†Ø´Ø¯',
                'job_id': job_id
            }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e),
            'job_id': job_id
        }, status=500)

def list_jobs(request):
    """Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… JobÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    try:
        if not os.path.exists('tmp_jobs'):
            return JsonResponse({'jobs': []})
        
        jobs = []
        for filename in os.listdir('tmp_jobs'):
            if filename.endswith('_status.json'):
                job_id = filename.replace('_status.json', '')
                try:
                    with open(f'tmp_jobs/{filename}', 'r', encoding='utf-8') as f:
                        status_data = json.load(f)
                        
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡
                        sites_count = 0
                        json_file = f'tmp_jobs/{job_id}.json'
                        if os.path.exists(json_file):
                            with open(json_file, 'r', encoding='utf-8') as f2:
                                json_data = json.load(f2)
                                products = json_data.get('products', [])
                                # ØªØ¹Ø¯Ø§Ø¯ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯
                                sites_count = len(set(p.get('site', '') for p in products if p.get('site')))
                        
                        jobs.append({
                            'job_id': job_id,
                            'status': status_data.get('status', 'Ù†Ø§Ù…Ø´Ø®Øµ'),
                            'products_count': status_data.get('total_products', 0),
                            'sites_count': sites_count,
                            'current_site': status_data.get('current_site', ''),
                            'timestamp': status_data.get('timestamp', '')
                        })
                except Exception as e:
                    print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ {filename}: {e}")
                    continue
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        jobs.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return JsonResponse({'jobs': jobs})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)})

def get_site_statistics(request, job_id):
    """Ø¢Ù…Ø§Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØª"""
    try:
        json_file = f'tmp_jobs/{job_id}.json'
        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            products = data.get('products', [])
            
            # Ø¢Ù…Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØª
            site_stats = {}
            for product in products:
                site = product.get('site', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                if site not in site_stats:
                    site_stats[site] = {
                        'count': 0,
                        'total_price': 0,
                        'min_price': float('inf'),
                        'max_price': 0
                    }
                
                site_stats[site]['count'] += 1
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚ÛŒÙ…Øª
                try:
                    price = int(product.get('price', 0))
                    site_stats[site]['total_price'] += price
                    site_stats[site]['min_price'] = min(site_stats[site]['min_price'], price)
                    site_stats[site]['max_price'] = max(site_stats[site]['max_price'], price)
                except:
                    pass
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
            for site in site_stats:
                if site_stats[site]['count'] > 0:
                    site_stats[site]['avg_price'] = site_stats[site]['total_price'] // site_stats[site]['count']
                else:
                    site_stats[site]['avg_price'] = 0
                
                # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
                if site_stats[site]['min_price'] == float('inf'):
                    site_stats[site]['min_price'] = 0
            
            return JsonResponse({
                'success': True,
                'job_id': job_id,
                'total_products': len(products),
                'total_sites': len(site_stats),
                'site_statistics': site_stats
            })
        else:
            return JsonResponse({
                'success': False,
                'error': 'ÙØ§ÛŒÙ„ Job ÛŒØ§ÙØª Ù†Ø´Ø¯'
            }, status=404)
            
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)

@csrf_exempt
def stop_scraping(request, job_id):
    """ØªÙˆÙ‚Ù ÛŒÚ© Job Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§"""
    if request.method == 'POST':
        try:
            # Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ú©Ø§Ù†ÛŒØ²Ù…ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ‚Ù Ø§Ø³Ú©Ø±Ù¾Ø± Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯
            # Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø±ØŒ ÙÙ‚Ø· ÙˆØ¶Ø¹ÛŒØª Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            status_file = f'tmp_jobs/{job_id}_status.json'
            if os.path.exists(status_file):
                with open(status_file, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                
                status_data['status'] = 'Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±'
                status_data['stopped'] = True
                
                with open(status_file, 'w', encoding='utf-8') as f:
                    json.dump(status_data, f, ensure_ascii=False, indent=2)
            
            return JsonResponse({
                'success': True,
                'message': 'Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆÙ‚Ù Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯',
                'job_id': job_id
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            }, status=500)
    
    return JsonResponse({'success': False, 'error': 'Ù…ØªØ¯ ØºÛŒØ±Ù…Ø¬Ø§Ø²'})

def get_supported_sites(request):
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡"""
    supported_sites = [
        {
            'name': 'Vape 60 Shop',
            'url': 'https://vape60shop22.com',
            'id': 'vape60'
        },
        {
            'name': 'Tajvape',
            'url': 'https://tajvape12.com',
            'id': 'tajvape'
        },
        {
            'name': 'Vapours Daily',
            'url': 'https://vapoursdaily14.com',
            'id': 'vapoursdaily'
        },
        {
            'name': 'Digi Zima',
            'url': 'https://digizima19.com',
            'id': 'digizima'
        },
        {
            'name': 'Smok Center',
            'url': 'https://smokcenter16.com',
            'id': 'smokcenter'
        },
        {
            'name': 'Digi Ghelioon',
            'url': 'https://digighelioon.com',
            'id': 'digighelioon'
        },
        {
            'name': 'Dokhan Market',
            'url': 'https://dokhanmarket3.com',
            'id': 'dokhanmarket'
        }
    ]
    
    return JsonResponse({
        'success': True,
        'sites': supported_sites,
        'total_sites': len(supported_sites)
    })